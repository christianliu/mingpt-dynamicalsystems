{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs26 \cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 system:\
    seed: 3407\
    work_dir: ./out/chargpt\
data:\
    block_size: 128\
model:\
    model_type: gpt-mini\
    n_layer: None\
    n_head: None\
    n_embd: None\
    vocab_size: None\
    block_size: None\
    embd_pdrop: 0.1\
    resid_pdrop: 0.1\
    attn_pdrop: 0.1\
trainer:\
    device: auto\
    num_workers: 4\
    max_iters: None\
    batch_size: 64\
    learning_rate: 0.0005\
    betas: (0.9, 0.95)\
    weight_decay: 0.1\
    grad_norm_clip: 1.0\
\
data has 5458199 characters, 91 unique.\
number of parameters: 2.71M\
running on device cuda\
/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\
  warnings.warn(\
iter_dt 0.00ms; iter 0: train loss 4.55656\
O God, O God! \}ReoteRtoo 6- eot u\
    :t   \} o\
  \
: \
:e\
\}ef:  o   e\
  toe R\
  -tR \}:t   \
:o: oe  o \}e\
\}R \
 \
-\
:\
ene\} :  \
  -Re\
,o\
 e \
  :- t: o \
te o :,  e \
\
\
R fo-   \
end   Re \
     oen\
o      e R,  t Rt t\
R :of:g t  \
 R \
 t\
  o :e eR9: : -o   o o\
e,t :go - -\}ot :o\
e  \
\} f   t       eR\
t\
o :toR oo -fgo-R\
9  \
eeRt  \
 -\
\
     \
o tRg\
e of\
 \
\},t R \}    -:te   \}: f\
e,oo \} \}\
9te,-  fe,R \
oof  :Reen oRg     \} t \
o:, \}\
eo :\
\} \}\
 \
  o -te e  e toe eRo  e\
ot  \}       ft  o oo o    :  :o  -:::f :o  \
 t\
ofo,   -,eeRe o\
saving model\
iter_dt 93.79ms; iter 10: train loss 3.26663\
iter_dt 96.52ms; iter 20: train loss 2.82429\
iter_dt 96.16ms; iter 30: train loss 2.68696\
iter_dt 94.70ms; iter 40: train loss 2.63809\
iter_dt 86.83ms; iter 50: train loss 2.51815\
iter_dt 104.96ms; iter 60: train loss 2.50495\
iter_dt 95.60ms; iter 70: train loss 2.47113\
iter_dt 91.31ms; iter 80: train loss 2.44553\
iter_dt 97.31ms; iter 90: train loss 2.40210\
iter_dt 95.95ms; iter 100: train loss 2.38269\
iter_dt 97.05ms; iter 110: train loss 2.40391\
iter_dt 98.01ms; iter 120: train loss 2.39397\
iter_dt 96.60ms; iter 130: train loss 2.37647\
iter_dt 96.77ms; iter 140: train loss 2.39703\
iter_dt 100.59ms; iter 150: train loss 2.32888\
iter_dt 99.47ms; iter 160: train loss 2.30774\
iter_dt 98.20ms; iter 170: train loss 2.29770\
iter_dt 96.40ms; iter 180: train loss 2.25554\
iter_dt 89.41ms; iter 190: train loss 2.26709\
iter_dt 105.00ms; iter 200: train loss 2.26520\
iter_dt 98.60ms; iter 210: train loss 2.27638\
iter_dt 102.17ms; iter 220: train loss 2.20097\
iter_dt 96.87ms; iter 230: train loss 2.25719\
iter_dt 97.39ms; iter 240: train loss 2.16070\
iter_dt 97.33ms; iter 250: train loss 2.20833\
iter_dt 97.56ms; iter 260: train loss 2.21478\
iter_dt 96.09ms; iter 270: train loss 2.18583\
iter_dt 98.42ms; iter 280: train loss 2.15714\
iter_dt 96.40ms; iter 290: train loss 2.16310\
iter_dt 99.42ms; iter 300: train loss 2.16297\
iter_dt 100.41ms; iter 310: train loss 2.12017\
iter_dt 99.36ms; iter 320: train loss 2.13026\
iter_dt 98.71ms; iter 330: train loss 2.08395\
iter_dt 94.24ms; iter 340: train loss 2.09108\
iter_dt 104.72ms; iter 350: train loss 2.10259\
iter_dt 96.82ms; iter 360: train loss 2.09076\
iter_dt 98.41ms; iter 370: train loss 2.02105\
iter_dt 98.32ms; iter 380: train loss 2.03050\
iter_dt 100.67ms; iter 390: train loss 2.01575\
iter_dt 99.42ms; iter 400: train loss 2.04641\
iter_dt 99.60ms; iter 410: train loss 1.99636\
iter_dt 97.98ms; iter 420: train loss 2.06805\
iter_dt 99.73ms; iter 430: train loss 1.95396\
iter_dt 100.74ms; iter 440: train loss 2.00131\
iter_dt 102.70ms; iter 450: train loss 2.00252\
iter_dt 115.01ms; iter 460: train loss 1.94937\
iter_dt 100.98ms; iter 470: train loss 1.95611\
iter_dt 101.46ms; iter 480: train loss 1.96112\
iter_dt 107.09ms; iter 490: train loss 1.95777\
iter_dt 99.94ms; iter 500: train loss 1.95206\
O God, O God!- I'll sharve my borne;\
    We wor then man is slay hade as for me.\
  MOSIN. The alie thy dor,\
    And of wor thad in thy nour to havelt ones sur\
    Our to his of my shardas thee fors, fore ithons anly of ither\
    are treeesst is brive hie thee suld and of they wa and,\
    I's mere of all but not we the now\
    A hat noter his, is so of oure bease some, of of to thice swore,\
    I stir is thrigh suive what.\
  Lent. A mig ond and to nowe the of word;\
      That seonee my hey love in to surter.\
\
saving model\
iter_dt 99.13ms; iter 510: train loss 1.94805\
iter_dt 101.53ms; iter 520: train loss 1.94341\
iter_dt 101.38ms; iter 530: train loss 1.91835\
iter_dt 99.09ms; iter 540: train loss 1.89287\
iter_dt 100.60ms; iter 550: train loss 1.83978\
iter_dt 101.41ms; iter 560: train loss 1.91369\
iter_dt 95.54ms; iter 570: train loss 1.89487\
iter_dt 112.47ms; iter 580: train loss 1.87976\
iter_dt 101.37ms; iter 590: train loss 1.84024\
iter_dt 106.08ms; iter 600: train loss 1.84669\
iter_dt 100.61ms; iter 610: train loss 1.85218\
iter_dt 98.88ms; iter 620: train loss 1.81806\
iter_dt 100.95ms; iter 630: train loss 1.91609\
iter_dt 102.05ms; iter 640: train loss 1.80381\
iter_dt 104.46ms; iter 650: train loss 1.84502\
iter_dt 100.68ms; iter 660: train loss 1.79611\
iter_dt 99.41ms; iter 670: train loss 1.76432\
iter_dt 103.86ms; iter 680: train loss 1.84047\
iter_dt 102.78ms; iter 690: train loss 1.77598\
iter_dt 96.97ms; iter 700: train loss 1.79195\
iter_dt 100.22ms; iter 710: train loss 1.77719\
iter_dt 100.25ms; iter 720: train loss 1.75815\
iter_dt 96.78ms; iter 730: train loss 1.74464\
iter_dt 102.59ms; iter 740: train loss 1.78730\
iter_dt 102.16ms; iter 750: train loss 1.72163\
iter_dt 101.10ms; iter 760: train loss 1.75926\
iter_dt 99.21ms; iter 770: train loss 1.69355\
iter_dt 100.94ms; iter 780: train loss 1.75505\
iter_dt 98.88ms; iter 790: train loss 1.76009\
iter_dt 90.06ms; iter 800: train loss 1.72436\
iter_dt 100.48ms; iter 810: train loss 1.74100\
iter_dt 100.10ms; iter 820: train loss 1.72962\
iter_dt 98.93ms; iter 830: train loss 1.70616\
iter_dt 101.59ms; iter 840: train loss 1.70599\
iter_dt 102.18ms; iter 850: train loss 1.69175\
iter_dt 107.74ms; iter 860: train loss 1.70136\
iter_dt 90.27ms; iter 870: train loss 1.68281\
iter_dt 99.14ms; iter 880: train loss 1.69375\
iter_dt 101.19ms; iter 890: train loss 1.64159\
iter_dt 100.43ms; iter 900: train loss 1.65594\
iter_dt 101.54ms; iter 910: train loss 1.68673\
iter_dt 99.54ms; iter 920: train loss 1.68951\
iter_dt 101.06ms; iter 930: train loss 1.71406\
iter_dt 100.37ms; iter 940: train loss 1.65655\
iter_dt 100.62ms; iter 950: train loss 1.63578\
iter_dt 102.92ms; iter 960: train loss 1.70004\
iter_dt 103.37ms; iter 970: train loss 1.63490\
iter_dt 101.56ms; iter 980: train loss 1.64047\
iter_dt 101.47ms; iter 990: train loss 1.61216\
iter_dt 94.17ms; iter 1000: train loss 1.63700\
O God, O God! When you\
                      "                             " Exit\
\
\
ACTNIUS. Gaous from straike more!\
  ANTINUS. Ha! Noish wraing if into, I'll you shonged you, the good\
  Flines, if you disble, them will hone sallicks in you budded a grac'd farls.\
  ARTHERSONA. Sholl she sor his wass the phich thee hath to danger!\
  APBETTRON. Afting the prest of hather him that.    [And they, thus weret our see some tong; I wak\
    come it witn mine as all inter matter\
  Atters a coment an had might sore wi\
saving model\
iter_dt 104.17ms; iter 1010: train loss 1.62275\
iter_dt 105.88ms; iter 1020: train loss 1.64386\
iter_dt 99.67ms; iter 1030: train loss 1.61246\
iter_dt 99.14ms; iter 1040: train loss 1.58963\
iter_dt 99.16ms; iter 1050: train loss 1.58996\
iter_dt 100.03ms; iter 1060: train loss 1.64127\
iter_dt 101.23ms; iter 1070: train loss 1.63322\
iter_dt 100.71ms; iter 1080: train loss 1.58237\
iter_dt 103.60ms; iter 1090: train loss 1.63332\
iter_dt 100.07ms; iter 1100: train loss 1.59463\
iter_dt 97.88ms; iter 1110: train loss 1.58616\
iter_dt 98.13ms; iter 1120: train loss 1.55692\
iter_dt 97.93ms; iter 1130: train loss 1.58333\
iter_dt 108.43ms; iter 1140: train loss 1.55006\
iter_dt 100.35ms; iter 1150: train loss 1.59388\
iter_dt 100.31ms; iter 1160: train loss 1.59988\
iter_dt 99.50ms; iter 1170: train loss 1.55859\
iter_dt 104.62ms; iter 1180: train loss 1.53332\
iter_dt 100.40ms; iter 1190: train loss 1.57975\
iter_dt 100.27ms; iter 1200: train loss 1.55851\
iter_dt 113.37ms; iter 1210: train loss 1.60574\
iter_dt 99.87ms; iter 1220: train loss 1.54171\
iter_dt 100.22ms; iter 1230: train loss 1.54330\
iter_dt 101.61ms; iter 1240: train loss 1.55389\
iter_dt 74.78ms; iter 1250: train loss 1.54771\
iter_dt 102.02ms; iter 1260: train loss 1.52836\
iter_dt 100.71ms; iter 1270: train loss 1.57691\
iter_dt 106.99ms; iter 1280: train loss 1.57763\
iter_dt 117.75ms; iter 1290: train loss 1.54338\
iter_dt 104.04ms; iter 1300: train loss 1.49686\
iter_dt 101.06ms; iter 1310: train loss 1.59075\
iter_dt 100.79ms; iter 1320: train loss 1.52234\
iter_dt 102.30ms; iter 1330: train loss 1.49644\
iter_dt 100.27ms; iter 1340: train loss 1.51644\
iter_dt 100.04ms; iter 1350: train loss 1.58091\
iter_dt 102.79ms; iter 1360: train loss 1.53024\
iter_dt 109.00ms; iter 1370: train loss 1.50978\
iter_dt 99.39ms; iter 1380: train loss 1.48747\
iter_dt 101.35ms; iter 1390: train loss 1.47784\
iter_dt 98.87ms; iter 1400: train loss 1.57505\
iter_dt 99.77ms; iter 1410: train loss 1.56287\
iter_dt 99.56ms; iter 1420: train loss 1.47584\
iter_dt 101.17ms; iter 1430: train loss 1.51429\
iter_dt 109.15ms; iter 1440: train loss 1.55553\
iter_dt 99.90ms; iter 1450: train loss 1.47996\
iter_dt 99.41ms; iter 1460: train loss 1.49548\
iter_dt 100.02ms; iter 1470: train loss 1.51026\
iter_dt 96.28ms; iter 1480: train loss 1.52270\
iter_dt 97.82ms; iter 1490: train loss 1.48686\
iter_dt 102.70ms; iter 1500: train loss 1.52957\
O God, O God!\
                                                      Exeunt\
\
\
SCENE III. They say and that that I teard.\
  FIRST CITIZEN. By them of him, and take a bose,\
    What is what a pessomer and alm of the stop,\
    Forter his did water to the pare hearts,\
    I'll suffectsion of the will one of thou\
    Sicceive of any for they hudly shop'd a were;\
    And which if you brawful of then temb texted hands,\
    Findiall'd my trips discrench the worsh,\
    Both of on't; he's heavy full trlees night-day yo\
saving model\
iter_dt 110.29ms; iter 1510: train loss 1.46034\
iter_dt 99.02ms; iter 1520: train loss 1.45185\
iter_dt 100.26ms; iter 1530: train loss 1.53791\
iter_dt 100.30ms; iter 1540: train loss 1.43390\
iter_dt 99.60ms; iter 1550: train loss 1.48353\
iter_dt 103.56ms; iter 1560: train loss 1.40624\
iter_dt 102.94ms; iter 1570: train loss 1.47362\
iter_dt 100.65ms; iter 1580: train loss 1.43515\
iter_dt 98.48ms; iter 1590: train loss 1.50470\
iter_dt 100.03ms; iter 1600: train loss 1.44505\
iter_dt 99.20ms; iter 1610: train loss 1.45749\
iter_dt 106.84ms; iter 1620: train loss 1.41765\
iter_dt 102.42ms; iter 1630: train loss 1.49618\
iter_dt 103.16ms; iter 1640: train loss 1.38740\
iter_dt 103.11ms; iter 1650: train loss 1.40548\
iter_dt 103.14ms; iter 1660: train loss 1.45429\
iter_dt 99.68ms; iter 1670: train loss 1.45570\
iter_dt 99.47ms; iter 1680: train loss 1.46054\
iter_dt 100.10ms; iter 1690: train loss 1.48182\
iter_dt 101.57ms; iter 1700: train loss 1.43625\
iter_dt 113.33ms; iter 1710: train loss 1.41428\
iter_dt 97.72ms; iter 1720: train loss 1.39266\
iter_dt 99.97ms; iter 1730: train loss 1.40383\
iter_dt 100.29ms; iter 1740: train loss 1.38932\
iter_dt 98.39ms; iter 1750: train loss 1.43254\
iter_dt 88.95ms; iter 1760: train loss 1.46240\
iter_dt 98.36ms; iter 1770: train loss 1.37807\
iter_dt 106.44ms; iter 1780: train loss 1.44303\
iter_dt 102.89ms; iter 1790: train loss 1.36558\
iter_dt 110.30ms; iter 1800: train loss 1.43416\
iter_dt 115.56ms; iter 1810: train loss 1.40204\
iter_dt 103.42ms; iter 1820: train loss 1.38494\
iter_dt 104.24ms; iter 1830: train loss 1.41849\
iter_dt 101.95ms; iter 1840: train loss 1.41033\
iter_dt 114.75ms; iter 1850: train loss 1.37452\
iter_dt 94.93ms; iter 1860: train loss 1.43810\
iter_dt 100.52ms; iter 1870: train loss 1.45412\
iter_dt 105.67ms; iter 1880: train loss 1.45646\
iter_dt 102.16ms; iter 1890: train loss 1.37313\
iter_dt 99.99ms; iter 1900: train loss 1.40156\
iter_dt 100.94ms; iter 1910: train loss 1.39617\
iter_dt 101.10ms; iter 1920: train loss 1.38529\
iter_dt 101.17ms; iter 1930: train loss 1.41703\
iter_dt 105.70ms; iter 1940: train loss 1.37107\
iter_dt 101.91ms; iter 1950: train loss 1.39211\
iter_dt 101.94ms; iter 1960: train loss 1.40618\
iter_dt 99.22ms; iter 1970: train loss 1.38148\
iter_dt 99.04ms; iter 1980: train loss 1.40280\
iter_dt 100.26ms; iter 1990: train loss 1.46157\
iter_dt 93.43ms; iter 2000: train loss 1.39022\
O God, O God!\
                                                       Exeunt.\
\
\
\
\
<<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM\
SHAKESPEARE IS COPYRIGHT 1990-193 BY WORLD LIBRARY, INC., AND IS\
PROVIDED BY PROJECT GUTENBERG ETED OR UTHE COPIES FOR YOUR UTHERS\
PERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED\
COMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY\
SERVICE THAT CHARGES FOR DOWNLOAD TIME OR USED\
COMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY AND MAYBELLE\
saving model\
iter_dt 98.57ms; iter 2010: train loss 1.41755\
iter_dt 95.93ms; iter 2020: train loss 1.37678\
iter_dt 100.70ms; iter 2030: train loss 1.36555\
iter_dt 100.50ms; iter 2040: train loss 1.38331\
iter_dt 101.99ms; iter 2050: train loss 1.37744\
iter_dt 96.92ms; iter 2060: train loss 1.42254\
iter_dt 98.37ms; iter 2070: train loss 1.36730\
iter_dt 99.18ms; iter 2080: train loss 1.37025\
iter_dt 99.70ms; iter 2090: train loss 1.38475\
iter_dt 92.26ms; iter 2100: train loss 1.44567\
iter_dt 94.36ms; iter 2110: train loss 1.37464\
iter_dt 105.68ms; iter 2120: train loss 1.38248\
iter_dt 102.82ms; iter 2130: train loss 1.38644\
iter_dt 100.33ms; iter 2140: train loss 1.39167\
iter_dt 99.63ms; iter 2150: train loss 1.41887\
iter_dt 98.60ms; iter 2160: train loss 1.37500\
iter_dt 99.36ms; iter 2170: train loss 1.36862\
iter_dt 98.86ms; iter 2180: train loss 1.38040\
iter_dt 97.44ms; iter 2190: train loss 1.38389\
iter_dt 100.06ms; iter 2200: train loss 1.36426\
iter_dt 99.31ms; iter 2210: train loss 1.38369\
iter_dt 113.83ms; iter 2220: train loss 1.35793\
iter_dt 102.50ms; iter 2230: train loss 1.37488\
iter_dt 104.92ms; iter 2240: train loss 1.34321\
iter_dt 99.28ms; iter 2250: train loss 1.36402\
iter_dt 101.59ms; iter 2260: train loss 1.40636\
iter_dt 104.38ms; iter 2270: train loss 1.35445\
iter_dt 105.01ms; iter 2280: train loss 1.30188\
iter_dt 100.89ms; iter 2290: train loss 1.31936\
iter_dt 101.52ms; iter 2300: train loss 1.32386\
iter_dt 101.03ms; iter 2310: train loss 1.32166\
iter_dt 100.91ms; iter 2320: train loss 1.35867\
iter_dt 103.90ms; iter 2330: train loss 1.36973\
iter_dt 102.40ms; iter 2340: train loss 1.37416\
iter_dt 99.83ms; iter 2350: train loss 1.34025\
iter_dt 100.47ms; iter 2360: train loss 1.35086\
iter_dt 100.02ms; iter 2370: train loss 1.36511\
iter_dt 97.80ms; iter 2380: train loss 1.33017\
iter_dt 100.73ms; iter 2390: train loss 1.36627\
iter_dt 95.40ms; iter 2400: train loss 1.40778\
iter_dt 90.68ms; iter 2410: train loss 1.30418\
iter_dt 103.15ms; iter 2420: train loss 1.42192\
iter_dt 100.43ms; iter 2430: train loss 1.39574\
iter_dt 103.63ms; iter 2440: train loss 1.36929\
iter_dt 114.58ms; iter 2450: train loss 1.30733\
iter_dt 112.00ms; iter 2460: train loss 1.37408\
iter_dt 100.77ms; iter 2470: train loss 1.29243\
iter_dt 100.10ms; iter 2480: train loss 1.36987\
iter_dt 101.05ms; iter 2490: train loss 1.33204\
iter_dt 102.78ms; iter 2500: train loss 1.36566\
O God, O God! I pray you will be gentleman.\
    It is your common business me?\
    We did not a mock army, as I will be merry a fair.\
    Your life, he we would make the world bloody to the water's langabour,\
     Thou shaltestill, I tarry to hear a went again,\
    And indeed my forest, I shall weep these come.\
    False the shape of an abscordament stand,\
    Yet mage to which male horse be not meant\
    By heart shall not with a band; but they for,\
    But in a send as tortune, what should have a more\
    \
saving model\
iter_dt 99.04ms; iter 2510: train loss 1.33159\
iter_dt 98.78ms; iter 2520: train loss 1.35765\
iter_dt 100.72ms; iter 2530: train loss 1.37032\
iter_dt 100.86ms; iter 2540: train loss 1.32959\
iter_dt 100.29ms; iter 2550: train loss 1.32823\
iter_dt 99.77ms; iter 2560: train loss 1.36674\
iter_dt 102.20ms; iter 2570: train loss 1.29244\
iter_dt 100.86ms; iter 2580: train loss 1.34867\
iter_dt 102.76ms; iter 2590: train loss 1.34140\
iter_dt 98.43ms; iter 2600: train loss 1.37842\
iter_dt 103.89ms; iter 2610: train loss 1.31660\
iter_dt 138.08ms; iter 2620: train loss 1.30221\
iter_dt 102.46ms; iter 2630: train loss 1.34811\
iter_dt 102.26ms; iter 2640: train loss 1.32511\
iter_dt 100.00ms; iter 2650: train loss 1.35283\
iter_dt 102.70ms; iter 2660: train loss 1.33755\
iter_dt 101.03ms; iter 2670: train loss 1.30365\
iter_dt 102.94ms; iter 2680: train loss 1.31283\
iter_dt 102.64ms; iter 2690: train loss 1.32639\
iter_dt 101.46ms; iter 2700: train loss 1.29456\
iter_dt 100.70ms; iter 2710: train loss 1.32204\
iter_dt 98.97ms; iter 2720: train loss 1.31977\
iter_dt 96.71ms; iter 2730: train loss 1.28610\
iter_dt 96.73ms; iter 2740: train loss 1.28198\
iter_dt 86.28ms; iter 2750: train loss 1.33263\
iter_dt 101.08ms; iter 2760: train loss 1.32843\
iter_dt 104.42ms; iter 2770: train loss 1.26330\
iter_dt 112.02ms; iter 2780: train loss 1.32870\
iter_dt 104.61ms; iter 2790: train loss 1.29463\
iter_dt 100.67ms; iter 2800: train loss 1.25851\
iter_dt 103.04ms; iter 2810: train loss 1.35474\
iter_dt 100.98ms; iter 2820: train loss 1.28567\
iter_dt 101.34ms; iter 2830: train loss 1.29625\
iter_dt 101.75ms; iter 2840: train loss 1.39384\
iter_dt 100.90ms; iter 2850: train loss 1.30791\
iter_dt 100.99ms; iter 2860: train loss 1.28824\
iter_dt 99.55ms; iter 2870: train loss 1.29527\
iter_dt 100.88ms; iter 2880: train loss 1.28281\
iter_dt 100.75ms; iter 2890: train loss 1.24974\
iter_dt 100.83ms; iter 2900: train loss 1.27413\
iter_dt 100.98ms; iter 2910: train loss 1.30404\
iter_dt 102.54ms; iter 2920: train loss 1.31658\
iter_dt 101.75ms; iter 2930: train loss 1.29724\
iter_dt 102.26ms; iter 2940: train loss 1.28402\
iter_dt 102.92ms; iter 2950: train loss 1.33136\
iter_dt 98.38ms; iter 2960: train loss 1.38612\
iter_dt 98.87ms; iter 2970: train loss 1.32301\
iter_dt 100.78ms; iter 2980: train loss 1.29618\
iter_dt 104.50ms; iter 2990: train loss 1.33081\
iter_dt 99.65ms; iter 3000: train loss 1.26971\
O God, O God!\
  SIR TOBY. Is I pay with that my fool play, thou shouldst thy man,\
    The straight of those cause matter for me? He let us be\
    will be most wild him, thou art a friended, where he lady\
    and birs armouried from his will.\
  HOST. He hard this such a wrong friend, strike her so subtle cut\
    his stage, and thy grace do not be gone.\
    Then her he is none.\
  HISTESS. How speaking he thou hast sir, and therefore that yet castle have defenc'd.\
  BOTTOM. O, if he was with it, his live, any f\
saving model\
iter_dt 96.03ms; iter 3010: train loss 1.23417\
iter_dt 95.80ms; iter 3020: train loss 1.28024\
iter_dt 102.26ms; iter 3030: train loss 1.31892\
iter_dt 100.04ms; iter 3040: train loss 1.31727\
iter_dt 100.91ms; iter 3050: train loss 1.31905\
iter_dt 101.02ms; iter 3060: train loss 1.26314\
iter_dt 99.35ms; iter 3070: train loss 1.26756\
iter_dt 100.22ms; iter 3080: train loss 1.33102\
iter_dt 99.53ms; iter 3090: train loss 1.31136\
iter_dt 100.12ms; iter 3100: train loss 1.32719\
iter_dt 98.71ms; iter 3110: train loss 1.30451\
iter_dt 105.16ms; iter 3120: train loss 1.29318\
iter_dt 108.65ms; iter 3130: train loss 1.32186\
iter_dt 100.72ms; iter 3140: train loss 1.33367\
iter_dt 99.90ms; iter 3150: train loss 1.26090\
iter_dt 102.46ms; iter 3160: train loss 1.29684\
iter_dt 103.53ms; iter 3170: train loss 1.30234\
iter_dt 100.81ms; iter 3180: train loss 1.31330\
iter_dt 99.30ms; iter 3190: train loss 1.21988\
iter_dt 101.72ms; iter 3200: train loss 1.27027\
iter_dt 100.57ms; iter 3210: train loss 1.27570\
iter_dt 102.80ms; iter 3220: train loss 1.31514\
iter_dt 99.93ms; iter 3230: train loss 1.30233\
iter_dt 100.86ms; iter 3240: train loss 1.34129\
iter_dt 99.57ms; iter 3250: train loss 1.26102\
iter_dt 100.23ms; iter 3260: train loss 1.24858\
iter_dt 99.44ms; iter 3270: train loss 1.27291\
iter_dt 100.55ms; iter 3280: train loss 1.24604\
iter_dt 102.43ms; iter 3290: train loss 1.27082\
iter_dt 102.18ms; iter 3300: train loss 1.27607\
iter_dt 100.78ms; iter 3310: train loss 1.26099\
iter_dt 101.36ms; iter 3320: train loss 1.29113\
iter_dt 100.56ms; iter 3330: train loss 1.23388\
iter_dt 101.05ms; iter 3340: train loss 1.33679\
iter_dt 100.24ms; iter 3350: train loss 1.29102\
iter_dt 102.58ms; iter 3360: train loss 1.27593\
iter_dt 106.97ms; iter 3370: train loss 1.32777\
iter_dt 100.30ms; iter 3380: train loss 1.25428\
iter_dt 92.66ms; iter 3390: train loss 1.28895\
iter_dt 101.91ms; iter 3400: train loss 1.24548\
iter_dt 101.58ms; iter 3410: train loss 1.33264\
iter_dt 101.06ms; iter 3420: train loss 1.35467\
iter_dt 108.50ms; iter 3430: train loss 1.34233\
iter_dt 109.14ms; iter 3440: train loss 1.28051\
iter_dt 100.34ms; iter 3450: train loss 1.29485\
iter_dt 101.91ms; iter 3460: train loss 1.29975\
iter_dt 100.06ms; iter 3470: train loss 1.27922\
iter_dt 98.75ms; iter 3480: train loss 1.21067\
iter_dt 98.94ms; iter 3490: train loss 1.32160\
iter_dt 90.93ms; iter 3500: train loss 1.29361\
O God, O God! here carry on at that I learn.\
  SAND GENTLEMAN. An it is my brother!\
  TITUS. I know her fair and mark'd my present will will,\
    But not with hence, and I am sorrywife\
    The host saying his whole fellow weights his souls blenz.\
                                                          [Exit\
\
\
\
SCENE III.\
Another true\
\
Enter BRUTUS. The KING RICHARD on this better him\
\
  HERALD. Thou didst bear him both in the blood. What are they?\
  PAGE. The fire of his men we shall speak word our groan. \
saving model\
iter_dt 102.91ms; iter 3510: train loss 1.22985\
iter_dt 101.21ms; iter 3520: train loss 1.18391\
iter_dt 99.35ms; iter 3530: train loss 1.27280\
iter_dt 98.07ms; iter 3540: train loss 1.29518\
iter_dt 102.28ms; iter 3550: train loss 1.26434\
iter_dt 100.32ms; iter 3560: train loss 1.28961\
iter_dt 99.61ms; iter 3570: train loss 1.23206\
iter_dt 100.91ms; iter 3580: train loss 1.23297\
iter_dt 96.78ms; iter 3590: train loss 1.26436\
iter_dt 99.22ms; iter 3600: train loss 1.24150\
iter_dt 126.49ms; iter 3610: train loss 1.26151\
iter_dt 105.53ms; iter 3620: train loss 1.26132\
iter_dt 99.86ms; iter 3630: train loss 1.28553\
iter_dt 99.61ms; iter 3640: train loss 1.29734\
iter_dt 100.74ms; iter 3650: train loss 1.23323\
iter_dt 101.53ms; iter 3660: train loss 1.21212\
iter_dt 102.81ms; iter 3670: train loss 1.26765\
iter_dt 88.88ms; iter 3680: train loss 1.27614\
iter_dt 102.12ms; iter 3690: train loss 1.27743\
iter_dt 101.07ms; iter 3700: train loss 1.25503\
iter_dt 101.61ms; iter 3710: train loss 1.19441\
iter_dt 100.56ms; iter 3720: train loss 1.25745\
iter_dt 97.09ms; iter 3730: train loss 1.26140\
iter_dt 97.99ms; iter 3740: train loss 1.26647\
iter_dt 81.78ms; iter 3750: train loss 1.28843\
iter_dt 99.09ms; iter 3760: train loss 1.22383\
iter_dt 111.22ms; iter 3770: train loss 1.25201\
iter_dt 104.53ms; iter 3780: train loss 1.25712\
iter_dt 104.46ms; iter 3790: train loss 1.25302\
iter_dt 102.84ms; iter 3800: train loss 1.32718\
iter_dt 100.65ms; iter 3810: train loss 1.24253\
iter_dt 100.75ms; iter 3820: train loss 1.23640\
iter_dt 99.58ms; iter 3830: train loss 1.23894\
iter_dt 99.13ms; iter 3840: train loss 1.26252\
iter_dt 99.75ms; iter 3850: train loss 1.25543\
iter_dt 111.61ms; iter 3860: train loss 1.21709\
iter_dt 101.50ms; iter 3870: train loss 1.22045\
iter_dt 100.09ms; iter 3880: train loss 1.21719\
iter_dt 105.03ms; iter 3890: train loss 1.24176\
iter_dt 101.86ms; iter 3900: train loss 1.21775\
iter_dt 100.36ms; iter 3910: train loss 1.26924\
iter_dt 100.37ms; iter 3920: train loss 1.31872\
iter_dt 100.55ms; iter 3930: train loss 1.24270\
iter_dt 100.71ms; iter 3940: train loss 1.21713\
iter_dt 101.09ms; iter 3950: train loss 1.20954\
iter_dt 101.03ms; iter 3960: train loss 1.30522\
iter_dt 103.53ms; iter 3970: train loss 1.24461\
iter_dt 100.71ms; iter 3980: train loss 1.23191\
iter_dt 99.53ms; iter 3990: train loss 1.23413\
iter_dt 99.48ms; iter 4000: train loss 1.27655\
O God, O God!\
    Out offirstand, brief fith, or secure, as your grace,\
    With those pity are colds. We'll go on't.\
    Macbeth sixth to that strangle show, how came is the\
      To drop our prest-battless; and sweet, music, be perforce but that\
promises, till the passion sovereign, flesh you\
    strike and am I should be face.\
\
                        Enter ANTIPHOLUS OF BROTUS and POSTHUMUS\
\
  TITISANIO. With a mirth of my work of moneys, and to determine\
    That traitor\
    In such subjects of the day\
\
saving model\
iter_dt 101.15ms; iter 4010: train loss 1.25578\
iter_dt 99.69ms; iter 4020: train loss 1.27944\
iter_dt 102.45ms; iter 4030: train loss 1.22200\
iter_dt 102.55ms; iter 4040: train loss 1.26888\
iter_dt 101.19ms; iter 4050: train loss 1.27283\
iter_dt 100.77ms; iter 4060: train loss 1.26488\
iter_dt 101.73ms; iter 4070: train loss 1.23216\
iter_dt 102.06ms; iter 4080: train loss 1.15137\
iter_dt 101.11ms; iter 4090: train loss 1.29515\
iter_dt 99.64ms; iter 4100: train loss 1.24839\
iter_dt 97.07ms; iter 4110: train loss 1.24868\
iter_dt 109.57ms; iter 4120: train loss 1.20114\
iter_dt 97.22ms; iter 4130: train loss 1.19347\
iter_dt 90.62ms; iter 4140: train loss 1.23884\
iter_dt 91.17ms; iter 4150: train loss 1.30717\
iter_dt 102.70ms; iter 4160: train loss 1.22173\
iter_dt 102.96ms; iter 4170: train loss 1.22069\
iter_dt 107.74ms; iter 4180: train loss 1.20523\
iter_dt 113.09ms; iter 4190: train loss 1.25260\
iter_dt 101.29ms; iter 4200: train loss 1.25748\
iter_dt 101.38ms; iter 4210: train loss 1.25772\
iter_dt 101.93ms; iter 4220: train loss 1.22347\
iter_dt 105.68ms; iter 4230: train loss 1.25269\
iter_dt 103.49ms; iter 4240: train loss 1.23635\
iter_dt 94.71ms; iter 4250: train loss 1.25842\
iter_dt 102.45ms; iter 4260: train loss 1.23288\
iter_dt 100.30ms; iter 4270: train loss 1.28172\
iter_dt 99.95ms; iter 4280: train loss 1.26088\
iter_dt 101.76ms; iter 4290: train loss 1.24432\
iter_dt 101.73ms; iter 4300: train loss 1.19183\
iter_dt 99.00ms; iter 4310: train loss 1.26480\
iter_dt 100.15ms; iter 4320: train loss 1.24468\
iter_dt 100.33ms; iter 4330: train loss 1.26835\
iter_dt 100.97ms; iter 4340: train loss 1.23592\
iter_dt 105.11ms; iter 4350: train loss 1.27238\
iter_dt 103.09ms; iter 4360: train loss 1.28922\
iter_dt 95.98ms; iter 4370: train loss 1.22731\
iter_dt 100.62ms; iter 4380: train loss 1.22337\
iter_dt 98.85ms; iter 4390: train loss 1.16579\
iter_dt 102.75ms; iter 4400: train loss 1.26458\
iter_dt 104.07ms; iter 4410: train loss 1.24710\
iter_dt 101.87ms; iter 4420: train loss 1.24435\
iter_dt 100.97ms; iter 4430: train loss 1.21736\
iter_dt 100.03ms; iter 4440: train loss 1.25311\
iter_dt 100.81ms; iter 4450: train loss 1.23111\
iter_dt 97.48ms; iter 4460: train loss 1.24746\
iter_dt 91.64ms; iter 4470: train loss 1.20770\
iter_dt 100.71ms; iter 4480: train loss 1.26108\
iter_dt 100.49ms; iter 4490: train loss 1.26688\
iter_dt 104.75ms; iter 4500: train loss 1.24468\
O God, O God!- what speed!'\
\
  LAUNCE. Think of war, be my patience? He should husband me\
    approach and tender the mannerable of a the form.\
  KING HENRY. Welcome!\
    Sad I cannot talk forth might, whose man should\
    To see here solemnity to me away. This, these deaths\
    Concerning honest son to high; and for her, be cordered,\
    The people was. By the purpose this debted soonest\
    And he secretion at the law pride servants.\
    What said to the merchant will the bold\
    Of an undask to like.\
  D\
saving model\
iter_dt 107.79ms; iter 4510: train loss 1.22094\
iter_dt 95.37ms; iter 4520: train loss 1.25488\
iter_dt 104.09ms; iter 4530: train loss 1.26157\
iter_dt 94.95ms; iter 4540: train loss 1.19938\
iter_dt 88.20ms; iter 4550: train loss 1.26297\
iter_dt 100.52ms; iter 4560: train loss 1.21133\
iter_dt 104.97ms; iter 4570: train loss 1.21537\
iter_dt 111.38ms; iter 4580: train loss 1.25443\
iter_dt 103.59ms; iter 4590: train loss 1.23819\
iter_dt 103.14ms; iter 4600: train loss 1.23387\
iter_dt 94.78ms; iter 4610: train loss 1.22721\
iter_dt 105.69ms; iter 4620: train loss 1.22852\
iter_dt 100.41ms; iter 4630: train loss 1.25727\
iter_dt 100.82ms; iter 4640: train loss 1.20926\
iter_dt 102.56ms; iter 4650: train loss 1.21179\
iter_dt 102.97ms; iter 4660: train loss 1.27494\
iter_dt 100.63ms; iter 4670: train loss 1.21303\
iter_dt 99.93ms; iter 4680: train loss 1.22403\
iter_dt 100.45ms; iter 4690: train loss 1.23351\
iter_dt 101.20ms; iter 4700: train loss 1.19048\
iter_dt 102.16ms; iter 4710: train loss 1.26372\
iter_dt 104.28ms; iter 4720: train loss 1.23953\
iter_dt 96.98ms; iter 4730: train loss 1.19155\
iter_dt 99.14ms; iter 4740: train loss 1.21622\
iter_dt 102.31ms; iter 4750: train loss 1.21119\
iter_dt 105.15ms; iter 4760: train loss 1.20868\
iter_dt 96.92ms; iter 4770: train loss 1.20244\
iter_dt 100.58ms; iter 4780: train loss 1.23447\
iter_dt 98.67ms; iter 4790: train loss 1.27478\
iter_dt 99.82ms; iter 4800: train loss 1.22242\
iter_dt 116.03ms; iter 4810: train loss 1.19012\
iter_dt 101.40ms; iter 4820: train loss 1.20430\
iter_dt 100.31ms; iter 4830: train loss 1.26371\
iter_dt 99.12ms; iter 4840: train loss 1.22769\
iter_dt 102.84ms; iter 4850: train loss 1.20493\
iter_dt 118.36ms; iter 4860: train loss 1.23980\
iter_dt 96.53ms; iter 4870: train loss 1.28777\
iter_dt 103.51ms; iter 4880: train loss 1.17843\
iter_dt 106.99ms; iter 4890: train loss 1.22293\
iter_dt 102.72ms; iter 4900: train loss 1.19621\
iter_dt 102.44ms; iter 4910: train loss 1.24333\
iter_dt 102.14ms; iter 4920: train loss 1.22517\
iter_dt 101.26ms; iter 4930: train loss 1.24767\
iter_dt 100.46ms; iter 4940: train loss 1.22804\
iter_dt 101.04ms; iter 4950: train loss 1.26770\
iter_dt 99.61ms; iter 4960: train loss 1.23274\
iter_dt 99.82ms; iter 4970: train loss 1.22396\
iter_dt 98.52ms; iter 4980: train loss 1.20493\
iter_dt 100.20ms; iter 4990: train loss 1.29272\
iter_dt 102.57ms; iter 5000: train loss 1.23012\
O God, O God!\
    I thought to dead by me as i' th' malice;\
    For my beauty, forest though I can report\
    About the track of the fantastions of the partnets,\
    Thy fool for spoke safe affections.  \
                                    Exeunt MENENIUS, with PAGE\
  CHIEF Justicellius aghes, but she stol'n me\
    And but the mistress fool at me any foot.\
  TIMON. Was not the worst? The plain is manhaliant?\
    Here, sweet thine of their achill, my lord, we'll say 'T.\
    Be a thousand of sun, it shall had \
saving model\
iter_dt 99.98ms; iter 5010: train loss 1.17331\
iter_dt 99.17ms; iter 5020: train loss 1.24418\
iter_dt 102.48ms; iter 5030: train loss 1.24451\
iter_dt 100.16ms; iter 5040: train loss 1.25989\
iter_dt 100.94ms; iter 5050: train loss 1.18008\
iter_dt 103.34ms; iter 5060: train loss 1.15470\
iter_dt 102.11ms; iter 5070: train loss 1.26129\
iter_dt 98.71ms; iter 5080: train loss 1.20157\
iter_dt 100.58ms; iter 5090: train loss 1.23833\
iter_dt 103.04ms; iter 5100: train loss 1.20539\
iter_dt 109.01ms; iter 5110: train loss 1.15741\
iter_dt 102.35ms; iter 5120: train loss 1.23593\
iter_dt 103.40ms; iter 5130: train loss 1.21854\
iter_dt 102.28ms; iter 5140: train loss 1.21552\
iter_dt 100.83ms; iter 5150: train loss 1.21213\
iter_dt 100.93ms; iter 5160: train loss 1.29028\
iter_dt 99.28ms; iter 5170: train loss 1.23814\
iter_dt 98.24ms; iter 5180: train loss 1.20696\
iter_dt 98.86ms; iter 5190: train loss 1.20166\
iter_dt 103.56ms; iter 5200: train loss 1.21770\
iter_dt 100.35ms; iter 5210: train loss 1.23096\
iter_dt 100.80ms; iter 5220: train loss 1.23276\
iter_dt 105.89ms; iter 5230: train loss 1.21742\
iter_dt 104.53ms; iter 5240: train loss 1.11897\
iter_dt 97.96ms; iter 5250: train loss 1.21999\
iter_dt 104.48ms; iter 5260: train loss 1.24845\
iter_dt 101.53ms; iter 5270: train loss 1.27465\
iter_dt 101.64ms; iter 5280: train loss 1.24358\
iter_dt 101.90ms; iter 5290: train loss 1.21022\
iter_dt 109.95ms; iter 5300: train loss 1.23390\
iter_dt 99.14ms; iter 5310: train loss 1.25546\
iter_dt 99.97ms; iter 5320: train loss 1.18659\
iter_dt 100.90ms; iter 5330: train loss 1.18629\
iter_dt 101.68ms; iter 5340: train loss 1.22405\
iter_dt 103.17ms; iter 5350: train loss 1.13984\
iter_dt 102.89ms; iter 5360: train loss 1.16948\
iter_dt 99.30ms; iter 5370: train loss 1.20306\
iter_dt 103.87ms; iter 5380: train loss 1.26328\
iter_dt 91.25ms; iter 5390: train loss 1.21541\
iter_dt 100.80ms; iter 5400: train loss 1.19329\
iter_dt 100.03ms; iter 5410: train loss 1.21383\
iter_dt 101.08ms; iter 5420: train loss 1.21431\
iter_dt 100.45ms; iter 5430: train loss 1.19951\
iter_dt 101.20ms; iter 5440: train loss 1.16853\
iter_dt 101.93ms; iter 5450: train loss 1.18651\
iter_dt 100.15ms; iter 5460: train loss 1.20828\
iter_dt 99.53ms; iter 5470: train loss 1.19342\
iter_dt 98.43ms; iter 5480: train loss 1.24483\
iter_dt 99.96ms; iter 5490: train loss 1.22047\
iter_dt 101.29ms; iter 5500: train loss 1.22812\
O God, O God! and you we are satisfy to send fire,\
    So ducats for time, my life, betwixt women-\
    Besides men and fend from my life and between thy\
    strangers' subjects and at her tole, by these many-\
    And with my pluck of mine end will do spirit\
    Hopes our fowur, trust the first part of her-lend out to be\
    Or noble strength, or the fiery that doth not be almost to meet.\
  FIRST WATCH. Stand that, I pray you, when away sometimister\
    hath then the further's partners do believe.\
  BOY. The \
saving model\
iter_dt 99.47ms; iter 5510: train loss 1.24710\
iter_dt 99.55ms; iter 5520: train loss 1.12190\
iter_dt 100.56ms; iter 5530: train loss 1.23605\
iter_dt 98.67ms; iter 5540: train loss 1.23495\
iter_dt 97.54ms; iter 5550: train loss 1.22508\
iter_dt 95.67ms; iter 5560: train loss 1.18330\
iter_dt 99.96ms; iter 5570: train loss 1.19678\
iter_dt 100.53ms; iter 5580: train loss 1.21911\
iter_dt 100.73ms; iter 5590: train loss 1.20671\
iter_dt 98.92ms; iter 5600: train loss 1.22716\
iter_dt 138.12ms; iter 5610: train loss 1.23110\
iter_dt 101.42ms; iter 5620: train loss 1.21326\
iter_dt 90.11ms; iter 5630: train loss 1.19424\
iter_dt 110.07ms; iter 5640: train loss 1.26512\
iter_dt 95.33ms; iter 5650: train loss 1.21761\
iter_dt 92.44ms; iter 5660: train loss 1.24846\
iter_dt 101.40ms; iter 5670: train loss 1.17632\
iter_dt 109.38ms; iter 5680: train loss 1.20810\
iter_dt 105.70ms; iter 5690: train loss 1.21420\
iter_dt 111.97ms; iter 5700: train loss 1.21804\
iter_dt 103.56ms; iter 5710: train loss 1.21082\
iter_dt 99.78ms; iter 5720: train loss 1.22690\
iter_dt 99.57ms; iter 5730: train loss 1.17231\
iter_dt 102.67ms; iter 5740: train loss 1.24322\
iter_dt 99.84ms; iter 5750: train loss 1.25005\
iter_dt 106.65ms; iter 5760: train loss 1.19531\
iter_dt 99.61ms; iter 5770: train loss 1.23717\
iter_dt 98.28ms; iter 5780: train loss 1.24047\
iter_dt 98.51ms; iter 5790: train loss 1.18010\
iter_dt 98.55ms; iter 5800: train loss 1.24394\
iter_dt 98.22ms; iter 5810: train loss 1.17395\
iter_dt 100.20ms; iter 5820: train loss 1.22067\
iter_dt 100.85ms; iter 5830: train loss 1.20866\
iter_dt 103.86ms; iter 5840: train loss 1.14518\
iter_dt 100.67ms; iter 5850: train loss 1.17079\
iter_dt 103.69ms; iter 5860: train loss 1.18182\
iter_dt 101.74ms; iter 5870: train loss 1.19695\
iter_dt 104.47ms; iter 5880: train loss 1.17173\
iter_dt 97.66ms; iter 5890: train loss 1.22788\
iter_dt 100.53ms; iter 5900: train loss 1.23299\
iter_dt 101.42ms; iter 5910: train loss 1.26655\
iter_dt 101.50ms; iter 5920: train loss 1.24503\
iter_dt 100.94ms; iter 5930: train loss 1.20210\
iter_dt 103.61ms; iter 5940: train loss 1.18786\
iter_dt 104.62ms; iter 5950: train loss 1.19959\
iter_dt 101.10ms; iter 5960: train loss 1.20130\
iter_dt 100.41ms; iter 5970: train loss 1.22727\
iter_dt 92.75ms; iter 5980: train loss 1.15828\
iter_dt 91.16ms; iter 5990: train loss 1.23789\
iter_dt 102.90ms; iter 6000: train loss 1.23831\
O God, O God! when the world set us him!\
    I saw him all was stone to clear,\
    This fashion as th' eye of as truth's bloods and the drugs,\
    As seeks will be down. By the world's having o' th' holp and assure\
    of most aspled, to her sister than blasking troubled, and held it\
    in her double- all himself, if you do, then beget\
    the sad ancient in your lady's five or fire,\
    And laying him with you, in loving the hour,\
    Which woman that sty both his pleasure cross.\
    Sun time's sufficience\
saving model\
iter_dt 103.95ms; iter 6010: train loss 1.13661\
iter_dt 99.33ms; iter 6020: train loss 1.18770\
iter_dt 96.96ms; iter 6030: train loss 1.19019\
iter_dt 91.18ms; iter 6040: train loss 1.20989\
iter_dt 98.61ms; iter 6050: train loss 1.14526\
iter_dt 101.16ms; iter 6060: train loss 1.18884\
iter_dt 100.57ms; iter 6070: train loss 1.16132\
iter_dt 103.46ms; iter 6080: train loss 1.16958\
iter_dt 109.20ms; iter 6090: train loss 1.17878\
iter_dt 108.55ms; iter 6100: train loss 1.19008\
iter_dt 125.79ms; iter 6110: train loss 1.18533\
iter_dt 93.77ms; iter 6120: train loss 1.18123\
iter_dt 93.95ms; iter 6130: train loss 1.17343\
iter_dt 100.79ms; iter 6140: train loss 1.19648\
iter_dt 104.37ms; iter 6150: train loss 1.18519\
iter_dt 105.02ms; iter 6160: train loss 1.21938\
iter_dt 101.78ms; iter 6170: train loss 1.16546\
iter_dt 101.99ms; iter 6180: train loss 1.20453\
iter_dt 99.65ms; iter 6190: train loss 1.19930\
iter_dt 100.65ms; iter 6200: train loss 1.16924\
iter_dt 99.75ms; iter 6210: train loss 1.18344\
iter_dt 101.20ms; iter 6220: train loss 1.23957\
iter_dt 91.39ms; iter 6230: train loss 1.16222\
iter_dt 108.19ms; iter 6240: train loss 1.20045\
iter_dt 92.11ms; iter 6250: train loss 1.19735\
iter_dt 106.88ms; iter 6260: train loss 1.22171\
iter_dt 101.79ms; iter 6270: train loss 1.18858\
iter_dt 100.14ms; iter 6280: train loss 1.20585\
iter_dt 100.78ms; iter 6290: train loss 1.21976\
iter_dt 104.63ms; iter 6300: train loss 1.19904\
iter_dt 101.75ms; iter 6310: train loss 1.19867\
iter_dt 101.09ms; iter 6320: train loss 1.17753\
iter_dt 100.30ms; iter 6330: train loss 1.21203\
iter_dt 99.85ms; iter 6340: train loss 1.20285\
iter_dt 98.47ms; iter 6350: train loss 1.20213\
iter_dt 88.65ms; iter 6360: train loss 1.12055\
iter_dt 100.23ms; iter 6370: train loss 1.24317\
iter_dt 110.66ms; iter 6380: train loss 1.15911\
iter_dt 105.38ms; iter 6390: train loss 1.21061\
iter_dt 100.70ms; iter 6400: train loss 1.21263\
iter_dt 101.74ms; iter 6410: train loss 1.22358\
iter_dt 103.54ms; iter 6420: train loss 1.12398\
iter_dt 102.41ms; iter 6430: train loss 1.10380\
iter_dt 103.17ms; iter 6440: train loss 1.20162\
iter_dt 101.45ms; iter 6450: train loss 1.20557\
iter_dt 97.25ms; iter 6460: train loss 1.21283\
iter_dt 101.13ms; iter 6470: train loss 1.18969\
iter_dt 102.17ms; iter 6480: train loss 1.15503\
iter_dt 100.70ms; iter 6490: train loss 1.19783\
iter_dt 98.18ms; iter 6500: train loss 1.19294\
O God, O God!\
    And I think that mere is a gruming minus,\
    That worse the surply boy, as it is.\
    A mour indirect, so well-same day I'll bring.\
    O heavy sinks; and say shame that ask it from the warrant?\
  DUCHESS. The text; the bleeding spring, for the said he deny his.\
  BATES, and she is not all the patient of my hearts;\
    But if I heard the late, the head hath all sound\
    With supple self-man that hears and born-bold on the supply fault;\
    For I must have sense the true crown that both to\
saving model\
iter_dt 104.43ms; iter 6510: train loss 1.18741\
iter_dt 97.84ms; iter 6520: train loss 1.16270\
iter_dt 98.76ms; iter 6530: train loss 1.18012\
iter_dt 98.69ms; iter 6540: train loss 1.14715\
iter_dt 101.17ms; iter 6550: train loss 1.17528\
iter_dt 100.67ms; iter 6560: train loss 1.19606\
iter_dt 102.36ms; iter 6570: train loss 1.14892\
iter_dt 99.72ms; iter 6580: train loss 1.23295\
iter_dt 95.54ms; iter 6590: train loss 1.16992\
iter_dt 103.76ms; iter 6600: train loss 1.19817\
iter_dt 116.65ms; iter 6610: train loss 1.19277\
iter_dt 99.38ms; iter 6620: train loss 1.17585\
iter_dt 102.75ms; iter 6630: train loss 1.21555\
iter_dt 101.31ms; iter 6640: train loss 1.22135\
iter_dt 101.68ms; iter 6650: train loss 1.21031\
iter_dt 100.69ms; iter 6660: train loss 1.11807\
iter_dt 103.47ms; iter 6670: train loss 1.24888\
iter_dt 101.18ms; iter 6680: train loss 1.21043\
iter_dt 101.33ms; iter 6690: train loss 1.20235\
iter_dt 100.07ms; iter 6700: train loss 1.20949\
iter_dt 100.23ms; iter 6710: train loss 1.16696\
iter_dt 99.49ms; iter 6720: train loss 1.22892\
iter_dt 84.60ms; iter 6730: train loss 1.16882\
iter_dt 94.28ms; iter 6740: train loss 1.11643\
iter_dt 119.06ms; iter 6750: train loss 1.15682\
iter_dt 102.41ms; iter 6760: train loss 1.17744\
iter_dt 100.09ms; iter 6770: train loss 1.21796\
iter_dt 100.75ms; iter 6780: train loss 1.14728\
iter_dt 100.19ms; iter 6790: train loss 1.14858\
iter_dt 100.79ms; iter 6800: train loss 1.17851\
iter_dt 102.74ms; iter 6810: train loss 1.17890\
iter_dt 102.83ms; iter 6820: train loss 1.17148\
iter_dt 102.25ms; iter 6830: train loss 1.20171\
iter_dt 101.83ms; iter 6840: train loss 1.11554\
iter_dt 100.74ms; iter 6850: train loss 1.18013\
iter_dt 112.67ms; iter 6860: train loss 1.18545\
iter_dt 101.29ms; iter 6870: train loss 1.17434\
iter_dt 100.90ms; iter 6880: train loss 1.20572\
iter_dt 107.27ms; iter 6890: train loss 1.20215\
iter_dt 99.91ms; iter 6900: train loss 1.18487\
iter_dt 103.55ms; iter 6910: train loss 1.16899\
iter_dt 100.24ms; iter 6920: train loss 1.19421\
iter_dt 101.38ms; iter 6930: train loss 1.16374\
iter_dt 101.02ms; iter 6940: train loss 1.14299\
iter_dt 100.08ms; iter 6950: train loss 1.18789\
iter_dt 100.97ms; iter 6960: train loss 1.20999\
iter_dt 99.17ms; iter 6970: train loss 1.15394\
iter_dt 99.69ms; iter 6980: train loss 1.17589\
iter_dt 100.97ms; iter 6990: train loss 1.18407\
iter_dt 104.27ms; iter 7000: train loss 1.18187\
O God, O God! this sweet Prince,\
    As he cannot bide that committed lovings!\
    Are to the same witness' children? But indeed,\
    And wonderful, and fair birrend on the soul?\
    If aught in place, stand, birds and things out of my side,\
    Must disdoy me as middling, hungerthrous shoulders;\
    And who choose her way was meted with me\
    As mother as sickness in her heart that traitors\
    Hath celeshes the fortune sick and hath bouns here\
    And submission to secret bare without.\
  AGRIPPA.' This he\
saving model\
iter_dt 100.41ms; iter 7010: train loss 1.17727\
iter_dt 100.62ms; iter 7020: train loss 1.21697\
iter_dt 99.05ms; iter 7030: train loss 1.16870\
iter_dt 100.23ms; iter 7040: train loss 1.22686\
iter_dt 100.55ms; iter 7050: train loss 1.17677\
iter_dt 100.22ms; iter 7060: train loss 1.14677\
iter_dt 100.12ms; iter 7070: train loss 1.20936\
iter_dt 101.28ms; iter 7080: train loss 1.17688\
iter_dt 98.05ms; iter 7090: train loss 1.15904\
iter_dt 97.82ms; iter 7100: train loss 1.21852\
iter_dt 91.69ms; iter 7110: train loss 1.21604\
iter_dt 89.23ms; iter 7120: train loss 1.16950\
iter_dt 100.77ms; iter 7130: train loss 1.18625\
iter_dt 108.88ms; iter 7140: train loss 1.18006\
iter_dt 109.47ms; iter 7150: train loss 1.18568\
iter_dt 101.48ms; iter 7160: train loss 1.21803\
iter_dt 100.36ms; iter 7170: train loss 1.17511\
iter_dt 103.03ms; iter 7180: train loss 1.14089\
iter_dt 101.65ms; iter 7190: train loss 1.19525\
iter_dt 101.15ms; iter 7200: train loss 1.20169\
iter_dt 100.29ms; iter 7210: train loss 1.23594\
iter_dt 103.18ms; iter 7220: train loss 1.16262\
iter_dt 99.15ms; iter 7230: train loss 1.14270\
iter_dt 99.55ms; iter 7240: train loss 1.16909\
iter_dt 100.45ms; iter 7250: train loss 1.15863\
iter_dt 101.30ms; iter 7260: train loss 1.21373\
iter_dt 98.91ms; iter 7270: train loss 1.12368\
iter_dt 100.27ms; iter 7280: train loss 1.22476\
iter_dt 100.24ms; iter 7290: train loss 1.14680\
iter_dt 107.52ms; iter 7300: train loss 1.13896\
iter_dt 102.33ms; iter 7310: train loss 1.19957\
iter_dt 101.23ms; iter 7320: train loss 1.17497\
iter_dt 103.13ms; iter 7330: train loss 1.16724\
iter_dt 100.06ms; iter 7340: train loss 1.14734\
iter_dt 100.30ms; iter 7350: train loss 1.18913\
iter_dt 114.65ms; iter 7360: train loss 1.12320\
iter_dt 104.99ms; iter 7370: train loss 1.17578\
iter_dt 100.55ms; iter 7380: train loss 1.17129\
iter_dt 110.25ms; iter 7390: train loss 1.15696\
iter_dt 101.07ms; iter 7400: train loss 1.21605\
iter_dt 102.22ms; iter 7410: train loss 1.17725\
iter_dt 104.74ms; iter 7420: train loss 1.15238\
iter_dt 102.47ms; iter 7430: train loss 1.20959\
iter_dt 97.18ms; iter 7440: train loss 1.17116\
iter_dt 89.70ms; iter 7450: train loss 1.15215\
iter_dt 97.90ms; iter 7460: train loss 1.15323\
iter_dt 102.22ms; iter 7470: train loss 1.16531\
iter_dt 100.65ms; iter 7480: train loss 1.13707\
iter_dt 110.05ms; iter 7490: train loss 1.14450\
iter_dt 112.23ms; iter 7500: train loss 1.23757\
O God, O God! this milk I weep so like.\
  FIRST MURDERER. What does this he hath not?\
  SIR TOBY. None a business, but this words being as by a blood in lowl\
    peer as sanching to the stock-fantash; which I word not to thee\
    on this moon in him; and therefore, what mouth the will\
    please theese princely blood and didst thou live;  \
    For all then they be the mouth to fold me and dream,\
    And sly my life and partly shall strike them.\
    Be answer'd away; now, no more, no, no more meet\
    To make\
saving model\
iter_dt 100.39ms; iter 7510: train loss 1.15185\
iter_dt 92.00ms; iter 7520: train loss 1.21057\
iter_dt 103.41ms; iter 7530: train loss 1.21819\
iter_dt 102.48ms; iter 7540: train loss 1.16894\
iter_dt 102.54ms; iter 7550: train loss 1.18477\
iter_dt 109.85ms; iter 7560: train loss 1.20056\
iter_dt 106.67ms; iter 7570: train loss 1.22008\
iter_dt 100.27ms; iter 7580: train loss 1.22411\
iter_dt 102.60ms; iter 7590: train loss 1.19264\
iter_dt 98.76ms; iter 7600: train loss 1.13107\
iter_dt 98.19ms; iter 7610: train loss 1.17013\
iter_dt 114.41ms; iter 7620: train loss 1.28010\
iter_dt 103.87ms; iter 7630: train loss 1.20115\
iter_dt 102.79ms; iter 7640: train loss 1.16522\
iter_dt 101.43ms; iter 7650: train loss 1.17935\
iter_dt 101.74ms; iter 7660: train loss 1.14658\
iter_dt 100.81ms; iter 7670: train loss 1.20467\
iter_dt 99.80ms; iter 7680: train loss 1.11984\
iter_dt 102.00ms; iter 7690: train loss 1.21006\
iter_dt 105.03ms; iter 7700: train loss 1.16737\
iter_dt 100.88ms; iter 7710: train loss 1.16518\
iter_dt 100.86ms; iter 7720: train loss 1.17337\
iter_dt 106.19ms; iter 7730: train loss 1.12818\
iter_dt 101.11ms; iter 7740: train loss 1.17312\
iter_dt 101.51ms; iter 7750: train loss 1.18650\
iter_dt 98.87ms; iter 7760: train loss 1.19915\
iter_dt 100.06ms; iter 7770: train loss 1.17618\
iter_dt 103.39ms; iter 7780: train loss 1.13876\
iter_dt 98.02ms; iter 7790: train loss 1.15243\
iter_dt 98.64ms; iter 7800: train loss 1.17080\
iter_dt 100.16ms; iter 7810: train loss 1.15367\
iter_dt 99.76ms; iter 7820: train loss 1.17557\
iter_dt 99.86ms; iter 7830: train loss 1.18299\
iter_dt 101.48ms; iter 7840: train loss 1.19405\
iter_dt 100.79ms; iter 7850: train loss 1.16181\
iter_dt 109.86ms; iter 7860: train loss 1.17414\
iter_dt 93.02ms; iter 7870: train loss 1.13914\
iter_dt 104.57ms; iter 7880: train loss 1.13719\
iter_dt 107.83ms; iter 7890: train loss 1.15667\
iter_dt 102.44ms; iter 7900: train loss 1.17312\
iter_dt 102.79ms; iter 7910: train loss 1.18628\
iter_dt 102.86ms; iter 7920: train loss 1.19603\
iter_dt 104.17ms; iter 7930: train loss 1.08449\
iter_dt 100.89ms; iter 7940: train loss 1.15845\
iter_dt 102.78ms; iter 7950: train loss 1.19875\
iter_dt 103.46ms; iter 7960: train loss 1.13121\
Traceback (most recent call last):\
  File "/content/drive/MyDrive/minGPT/projects/chargpt/chargpt.py", line 133, in <module>\
    trainer.run()\
  File "/content/drive/MyDrive/minGPT/mingpt/trainer.py", line 89, in run\
    batch = [t.to(self.device) for t in batch]\
             ^^^^^^^^^^^^^^^^^\
KeyboardInterrupt\
}